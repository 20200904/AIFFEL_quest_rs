{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e45ee0e3",
   "metadata": {},
   "source": [
    "#### 코드 출처 : https://www.kaggle.com/code/donariumdebbie/kakr-2nd-dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2219da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "# from keras import models\n",
    "# from keras import layers\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a944f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('~/aiffel/house-price-prediction_kaggle/data/train.csv', parse_dates=['date'])\n",
    "test  = pd.read_csv('~/aiffel/house-price-prediction_kaggle/data/test.csv', parse_dates=['date'], index_col='id')\n",
    "train.head()\n",
    "train_y = train[\"price\"].copy()\n",
    "\n",
    "## feature engineering 한번에 하는 함수 \n",
    "### train/test split, \n",
    "\n",
    "def feature_engineering(df, is_train = True):\n",
    "    \n",
    "    # feature 1 : sum and sub of latitude longitude \n",
    "    df['latlongsum'] = df['lat'] + df['long']\n",
    "    df['latlongsub'] = df['lat'] - df['long']\n",
    "    \n",
    "    # feature 2 : month and year\n",
    "    df['Month'] = df['date'].dt.month\n",
    "    df['Year'] = df['date'].dt.year\n",
    "    \n",
    "    # feature 3 : renovated year update\n",
    "    df.loc[df.yr_renovated==0,'yr_renovated']=df[df.yr_renovated==0].yr_built\n",
    "    \n",
    "    # feature 4 : zipfeatures (ref: https://www.kaggle.com/tmheo74/geo-data-eda-and-feature-engineering)\n",
    "    df['zipcode_str'] = df['zipcode'].astype(str)  \n",
    "    df['zipcode-3'] = 'z_' + df['zipcode_str'].str[2:3]\n",
    "    df['zipcode-4'] = 'z_' + df['zipcode_str'].str[3:4]\n",
    "    df['zipcode-5'] = 'z_' + df['zipcode_str'].str[4:5]\n",
    "    df['zipcode-34'] = 'z_' + df['zipcode_str'].str[2:4]\n",
    "    df['zipcode-45'] = 'z_' + df['zipcode_str'].str[3:5]\n",
    "    df['zipcode-35'] = 'z_' + df['zipcode_str'].str[2:3] + df['zipcode_str'].str[4:5]\n",
    "    df.drop(['zipcode_str'], 1, inplace=True)\n",
    "    \n",
    "    # drop useless columns\n",
    "    if is_train:\n",
    "        df.drop([\"id\"], 1, inplace=True)\n",
    "        df.drop([\"price\"], 1, inplace=True)\n",
    "        df.drop([\"date\"], 1, inplace=True)\n",
    "    else: # test는 id랑 price가 없으므로\n",
    "        df.drop([\"date\"], 1, inplace=True)\n",
    "    \n",
    "    # label encoding\n",
    "    cat_cols = df.select_dtypes('object').columns\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        \n",
    "    # feature 5 : pca -> date 드랍후에 해야됨. \n",
    "    pca1 = PCA(n_components=2)\n",
    "    pca1.fit(df)\n",
    "    coord_pca2 = pca1.transform(df)\n",
    "    df['pca1'] = coord_pca2[:, 0]\n",
    "    df['pca2'] = coord_pca2[:, 1]\n",
    "    \n",
    "    # feature 6 : pca (lat, long)\n",
    "    coord = df[['lat','long']]\n",
    "    pca2 = PCA(n_components=2)\n",
    "    pca2.fit(coord)\n",
    "    coord_pca = pca2.transform(coord)\n",
    "    df['coord_pca1'] = coord_pca[:, 0]\n",
    "    df['coord_pca2'] = coord_pca[:, 1]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ## 평가함수 \n",
    "# def eval(val_y, pred):\n",
    "#     rmse = np.sqrt(mse(val_y, pred))\n",
    "#     return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45ce430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = feature_engineering(train, True)\n",
    "test = feature_engineering(test, False)\n",
    "\n",
    "def rmse_exp(predictions, dmat):\n",
    "    labels = dmat.get_label()\n",
    "    error = np.expm1(predictions) - np.expm1(labels)\n",
    "    squared_error = np.square(error)\n",
    "    mean = np.mean(squared_error)\n",
    "    return ('rmse_exp', np.sqrt(mean))\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.02,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.4,\n",
    "    'objective': 'reg:linear',    \n",
    "    'eval_metric': 'rmse',        \n",
    "    'silent': True,               \n",
    "    'n_estimators' : 100\n",
    "}\n",
    "\n",
    "train_y = np.log1p(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28fb302e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:34:10] WARNING: ../src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[04:34:10] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"n_estimators\", \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:34:10] WARNING: ../src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[04:34:10] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"n_estimators\", \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:34:11] WARNING: ../src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[04:34:11] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"n_estimators\", \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:34:11] WARNING: ../src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[04:34:11] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"n_estimators\", \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[04:34:11] WARNING: ../src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[04:34:11] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"n_estimators\", \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\n",
      "Best Rounds: 2269\n",
      "Best Score: 112265.21\n",
      "CPU times: user 7min 19s, sys: 1.16 s, total: 7min 20s\n",
      "Wall time: 3min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# transforming\n",
    "dtrain = xgb.DMatrix(train, train_y)\n",
    "dtest = xgb.DMatrix(test)\n",
    "\n",
    "# cross validation\n",
    "cv_output = xgb.cv(xgb_params,\n",
    "                   dtrain,                        \n",
    "                   num_boost_round=10000,        \n",
    "                   early_stopping_rounds=200,    \n",
    "                   nfold=5,                      \n",
    "                   verbose_eval=200,             \n",
    "                   feval=rmse_exp,               \n",
    "                   maximize=False,\n",
    "                   show_stdv=False,\n",
    "                   seed = 1080\n",
    "                   )\n",
    "\n",
    "# scoring\n",
    "best_rounds = cv_output.index.size\n",
    "score = round(cv_output.iloc[-1]['test-rmse_exp-mean'], 2)\n",
    "\n",
    "print(f'\\nBest Rounds: {best_rounds}')\n",
    "print(f'Best Score: {score}')\n",
    "\n",
    "# plotting\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,5))\n",
    "# cv_output[['train-rmse-mean', 'test-rmse-mean']].plot(ax=ax1)\n",
    "# ax1.set_title('RMSE_log', fontsize=20)\n",
    "# cv_output[['train-rmse_exp-mean', 'test-rmse_exp-mean']].plot(ax=ax2)\n",
    "# ax2.set_title('RMSE', fontsize=20)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028d040e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:37:52] WARNING: ../src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[04:37:52] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"n_estimators\", \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(xgb_params, dtrain, num_boost_round=best_rounds)\n",
    "y_pred = model.predict(dtest)\n",
    "y_pred_xgb = np.expm1(y_pred)\n",
    "print(y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75476ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"~/aiffel/house-price-prediction_kaggle/data/sample_submission.csv\") \n",
    "sub1 = sub.copy()\n",
    "sub1['price'] = y_pred_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26243cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허태명님 커널참고하며 조금 바꿈\n",
    "\n",
    "# 데이터 가져오기\n",
    "def load_original_data():\n",
    "    train = pd.read_csv('~/aiffel/house-price-prediction_kaggle/data/train.csv', parse_dates=['date'])\n",
    "    test = pd.read_csv('~/aiffel/house-price-prediction_kaggle/data/test.csv', parse_dates=['date'], index_col='id')\n",
    "\n",
    "    train_copy = train.copy()\n",
    "    train_copy['data'] = 'train'\n",
    "    test_copy = test.copy()\n",
    "    test_copy['data'] = 'test'\n",
    "    test_copy['price'] = np.nan\n",
    "\n",
    "    # remove outlier\n",
    "    train_copy = train_copy[~((train_copy['sqft_living'] > 12000) & (train_copy['price'] < 3000000))].reset_index(drop=True)\n",
    "\n",
    "    # concat train, test data to preprocess\n",
    "    data = pd.concat([train_copy, test_copy], sort=False).reset_index(drop=True)\n",
    "    data = data[train_copy.columns]\n",
    "\n",
    "    # 날짜피쳐 드랍전에 중요한거 추가\n",
    "    data['Month'] = data['date'].dt.month\n",
    "    data['Year'] = data['date'].dt.year\n",
    "    \n",
    "    data.drop('date', axis=1, inplace=True)\n",
    "    data['zipcode'] = data['zipcode'].astype(str)\n",
    "\n",
    "    # fix skew feature\n",
    "    skew_columns = ['price']\n",
    "\n",
    "    for c in skew_columns:\n",
    "        data[c] = np.log1p(data[c])\n",
    "        \n",
    "    return data\n",
    "\n",
    "RANDOM_SEED = 1080\n",
    "np.random.seed(RANDOM_SEED)\n",
    "def rmse_exp(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(np.expm1(y_true), np.expm1(y_pred)))\n",
    "\n",
    "def train_test_split(data, do_ohe=True):\n",
    "    df = data.drop(['id','price','data'], axis=1).copy()\n",
    "    cat_cols = df.select_dtypes('object').columns\n",
    "    for col in cat_cols:\n",
    "        if do_ohe:\n",
    "            ohe_df = pd.get_dummies(df[[col]], prefix='ohe_'+col)\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "            df = pd.concat([df, ohe_df], axis=1)\n",
    "        else:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "\n",
    "    train_len = data[data['data'] == 'train'].shape[0]\n",
    "    X_train = df.iloc[:train_len]\n",
    "    X_test = df.iloc[train_len:]\n",
    "    y_train = data[data['data'] == 'train']['price']\n",
    "    \n",
    "    return X_train, X_test, y_train\n",
    "\n",
    "\n",
    "def get_oof_lgb(X_train, y_train, X_test, lgb_param, verbose_eval=False, return_cv_score_only=False):\n",
    "\n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "    oof = np.zeros(len(X_train))\n",
    "    predictions = np.zeros(len(X_test))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train.values, y_train.values)):\n",
    "        if verbose_eval > 0: print(f'Fold : {fold_ + 1}')\n",
    "        trn_data = lgb.Dataset(X_train.iloc[trn_idx], label=y_train.iloc[trn_idx])\n",
    "        val_data = lgb.Dataset(X_train.iloc[val_idx], label=y_train.iloc[val_idx])\n",
    "\n",
    "        num_round = 100000\n",
    "        clf = lgb.train(lgb_param, trn_data, num_round, valid_sets=[trn_data, val_data],\n",
    "                        verbose_eval=verbose_eval, early_stopping_rounds=200)\n",
    "        oof[val_idx] = clf.predict(X_train.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "        predictions += clf.predict(X_test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "        \n",
    "        cv_fold_score = rmse_exp(y_train.iloc[val_idx], oof[val_idx])\n",
    "        \n",
    "        if verbose_eval > 0: print(f'Fold {fold_ + 1} / CV-Score: {cv_fold_score:.6f}')\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = X_train.columns.tolist()\n",
    "        fold_importance_df['importance'] = clf.feature_importance('gain')\n",
    "        fold_importance_df['fold'] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "    cv_score = rmse_exp(y_train, oof)\n",
    "    print(f'CV-Score: {cv_score:.6f}')\n",
    "    if return_cv_score_only: return cv_score\n",
    "    else: return oof, predictions, cv_score, feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43361a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_original_data()\n",
    "\n",
    "coord = data[['lat','long']]\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(coord)\n",
    "\n",
    "coord_pca = pca.transform(coord)\n",
    "\n",
    "data['coord_pca1'] = coord_pca[:, 0]\n",
    "data['coord_pca2'] = coord_pca[:, 1]\n",
    "\n",
    "# 피쳐추가 \n",
    "data['latlongsum'] = data['lat'] + data['long']\n",
    "data['latlongsub'] = data['lat'] - data['long']\n",
    "data.loc[data.yr_renovated==0,'yr_renovated'] = data[data.yr_renovated==0].yr_built\n",
    "\n",
    "# kmeans for lat, long\n",
    "kmeans = KMeans(n_clusters=32, random_state=RANDOM_SEED).fit(coord)\n",
    "coord_cluster = kmeans.predict(coord)\n",
    "data['coord_cluster'] = coord_cluster\n",
    "data['coord_cluster'] = data['coord_cluster'].map(lambda x: 'c_' + str(x).rjust(2, '0'))\n",
    "\n",
    "X_train, X_test, y_train = train_test_split(data, do_ohe = False)\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "lgb_param = {\n",
    "    'objective': 'regression',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 15,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction': 0.7,\n",
    "    'seed': RANDOM_SEED,\n",
    "    'metric': ['rmse'],\n",
    "}\n",
    "\n",
    "oof, lgbm_pred, cv_score, fi_df = get_oof_lgb(X_train, y_train, X_test, lgb_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f474fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lgbm = np.expm1(lgbm_pred)\n",
    "sub2 = pd.read_csv(\"~/aiffel/house-price-prediction_kaggle/data/sample_submission.csv\") \n",
    "sub2['price'] = y_pred_lgbm\n",
    "# pred_df = sub.copy()\n",
    "pred_df = pd.read_csv(\"~/aiffel/house-price-prediction_kaggle/data/sample_submission.csv\") \n",
    "pred_df['price'] = sub1['price']*0.55 + sub2['price']*0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e64db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export(pred):\n",
    "    subm = pd.read_csv('~/aiffel/house-price-prediction_kaggle/data/sample_submission.csv')\n",
    "    subm['price'] = pred\n",
    "\n",
    "    subm_num = 0\n",
    "    subm_name = './subm_{}.csv'.format(str(subm_num).zfill(3))\n",
    "\n",
    "    while os.path.isfile(subm_name):\n",
    "        subm_num += 1\n",
    "        subm_name = './subm_{}.csv'.format(str(subm_num).zfill(3))\n",
    "\n",
    "    print(subm_name)\n",
    "    subm.to_csv(subm_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ba481",
   "metadata": {},
   "outputs": [],
   "source": [
    "export(pred_df['price'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
